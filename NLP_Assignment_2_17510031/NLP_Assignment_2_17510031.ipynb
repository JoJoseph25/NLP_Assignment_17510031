{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nASSIGNMENT 2\\n\\nNAME- JOEL V JOSEPH\\n\\nROLL NO- 17510031\\n\\nM.Sc Cognitive Science\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ASSIGNMENT 2\n",
    "\n",
    "NAME- JOEL V JOSEPH\n",
    "\n",
    "ROLL NO- 17510031\n",
    "\n",
    "M.Sc Cognitive Science\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msplit(docs, sep):\n",
    "    if type(docs)!=type([]):\n",
    "        doc=[docs]\n",
    "    for char in sep:\n",
    "        words = []\n",
    "        for strings in docs:\n",
    "            words.extend(strings.split(char))\n",
    "        docs = words\n",
    "    return docs\n",
    "\n",
    "def inverse_dict(d):\n",
    "    invr_dict={}\n",
    "    for key in d:\n",
    "        check = invr_dict.get(d[key],0)\n",
    "        if check==0:\n",
    "            invr_dict[d[key]] = [key]\n",
    "        else:\n",
    "            invr_dict[d[key]].append(key)\n",
    "    return invr_dict\n",
    "\n",
    "def histogram(word_list):\n",
    "    # word_list=list(s.split())\n",
    "    word_dict= {}\n",
    "    for word in word_list:\n",
    "        word_dict[word]= word_dict.get(word,0)+1\n",
    "    return word_dict\n",
    "\n",
    "def word_freq_descending(L):\n",
    "    d1={}\n",
    "    for word in L:\n",
    "        d1[word] = d1.get(word,0) + 1\n",
    "    inverse_d1= inverse_dict(d1)\n",
    "    sorted_freq=list(inverse_d1.keys())\n",
    "    sorted_freq.sort()\n",
    "    sorted_freq=sorted_freq[::-1]\n",
    "    Final_list=[]\n",
    "    for freq in sorted_freq:\n",
    "        repeat= freq\n",
    "        for word in inverse_d1[freq]:\n",
    "            for i in range(repeat):\n",
    "                Final_list.append(word)\n",
    "    word_dict=histogram(Final_list)\n",
    "    return word_dict\n",
    "\n",
    "def count_vocab(list_words):\n",
    "    N = []\n",
    "    i=0\n",
    "    V=[]\n",
    "    v = set()\n",
    "    for word in words:\n",
    "        i=i+1\n",
    "        if word not in V:\n",
    "            v.add(word)\n",
    "        V.append(len(v))\n",
    "        N.append(i)\n",
    "        #print(\"Computing Vocab \"+ str(int(i/len(words)*100))+\"%\"+\" complete\")\n",
    "    return N,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1 + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Alice in Wonderland and segmenting each word into list and creating a word frequency dictonary\n",
    "'''\n",
    "\n",
    "#Read the doc\n",
    "file = open(\"Alice_in_Wonderland.txt\", 'r')\n",
    "doc=file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# Replace new line character and \"* with \" \"\n",
    "def char_replace(doc,chars):\n",
    "    for char in chars:\n",
    "        while char in doc:\n",
    "            doc = doc.replace(char,\" \")\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "# Remove unnecesaary symbols\n",
    "def char_remove(docs,sep):\n",
    "    new_doc=[None]*len(docs)\n",
    "    \n",
    "    if type(docs)!=type([]):\n",
    "        doc=[docs]\n",
    "        \n",
    "    for i in range(0,len(sep)):\n",
    "        char=sep[i]\n",
    "        for j in range(0,len(docs)):\n",
    "            if i ==0:\n",
    "                new_doc[j]=docs[j].replace(char,\"\")\n",
    "            else:\n",
    "                new_doc[j]=new_doc[j].replace(char,\"\")\n",
    "\n",
    "    return new_doc\n",
    "# tokenize document into sentences + remove unnecessary symbols\n",
    "doc=char_replace(doc,(\"\\n\",\"*\"))\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(doc)\n",
    "sepraters=[\"\\n\",\",\",\"(\",\")\",\"--\",\"\\\"\",\"[\",\"]\",\"!\",\"?\",\"_\",\".\",\";\",\":\",\"”\",\"“\",\"*\",\",\",\"‘\",\",\",\"’\"]\n",
    "sentences=char_remove(sent_tokenize_list,sepraters)\n",
    "sentences=list(filter(lambda x: x != \"\", sentences))\n",
    "\n",
    "# mark start<s> and end</s> of sentences\n",
    "def strt_end_mark(sentences):\n",
    "    i=0\n",
    "    for sentence in sentences:\n",
    "        new_sent= \"<s> \"+sentence+\" </s>\"\n",
    "        sentences[i]=new_sent\n",
    "        i+=1\n",
    "    return sentences\n",
    "sentences=strt_end_mark(sentences)\n",
    "\n",
    "\n",
    "#split the doc into 80 train and 20 test\n",
    "random.seed(25)\n",
    "data=random.sample(sentences,len(sentences))\n",
    "train_data=data[:782]\n",
    "test_data=data[782:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary:  22622\n",
      "Tokens:  2828 \n",
      "\n",
      "Number of Actual Bigrams:  21840\n",
      "Number of Possible Bigrams:  7994756\n",
      "Percentage of Bigrams in the document:  0.2731790688796506 \n",
      "\n",
      "Number of Actual Trigrams:  21058\n",
      "Number of Possible Trirams:  22593180456\n",
      "Percentage of Trigrams in the document:  9.320511576938117e-05 \n",
      "\n",
      "Number of Actual Quadgrams:  20276\n",
      "Number of Possible Quadrams:  63825734788200\n",
      "Percentage of Quadgrams in the document:  3.176775021436744e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unigram MLE Prob\n",
    "words=msplit(train_data,(\" \"))\n",
    "words=list(filter(lambda x: x != \"\", words))\n",
    "\n",
    "\n",
    "unigram_dict=word_freq_descending(words)\n",
    "N= len(list(unigram_dict.keys()))\n",
    "V= sum(unigram_dict.values())\n",
    "unigram_MLE={}\n",
    "for key,value in unigram_dict.items():\n",
    "    unigram_MLE[key]=value/N\n",
    "\n",
    "\n",
    "print(\"Length of Vocabulary: \", V)\n",
    "print(\"Tokens: \", N, \"\\n\")\n",
    "\n",
    "# spliting sentences into n grams provided n (2- for bigram , 3- for tigram)\n",
    "def n_gram_split(sent,n):\n",
    "    n_grams=[]\n",
    "    if n<=1:\n",
    "        print(\"Use msplit with \\\"\\\" \")\n",
    "        return None\n",
    "    if type(sent)!=type([]):\n",
    "        sent=[sent]\n",
    "    words=msplit(sent,(\" \"))\n",
    "    words=list(filter(lambda x: x != \"\", words))\n",
    "    for i in range(0,len(words)-(n-1)):\n",
    "        n_gram=[]\n",
    "        for j in range(0,n):\n",
    "            n_gram.append(words[i+j])\n",
    "        n_grams.append(\" \".join(n_gram))\n",
    "    n_grams=list(filter(lambda x: \"</s> <s>\" not in x, n_grams))\n",
    "    return n_grams\n",
    "\n",
    "# Bigram MLE Prob\n",
    "bigrams=n_gram_split(train_data,2)\n",
    "\n",
    "bigram_dict=word_freq_descending(bigrams)\n",
    "\n",
    "bigram_MLE={}\n",
    "for key,value in bigram_dict.items():\n",
    "    word_1, word_2=key.split(\" \")\n",
    "    bigram_MLE[key]=value/unigram_dict[word_1]\n",
    "\n",
    "print(\"Number of Actual Bigrams: \",len(bigrams))\n",
    "poss=int(N*(N-1))\n",
    "print(\"Number of Possible Bigrams: \",poss)\n",
    "print(\"Percentage of Bigrams in the document: \",len(bigrams)/poss*100,\"\\n\")\n",
    "\n",
    "# Trigrams MLE Prob\n",
    "trigrams=n_gram_split(train_data,3)\n",
    "\n",
    "trigram_dict=word_freq_descending(trigrams)\n",
    "\n",
    "\n",
    "trigram_MLE={}\n",
    "for key,value in trigram_dict.items():\n",
    "    word_1, word_2, word_3=key.split(\" \")\n",
    "    trigram_MLE[key]=value/bigram_dict.get(word_1+\" \"+word_2)\n",
    "\n",
    "print(\"Number of Actual Trigrams: \",len(trigrams))\n",
    "poss=int(N*(N-1)*(N-2))\n",
    "print(\"Number of Possible Trirams: \",poss)\n",
    "print(\"Percentage of Trigrams in the document: \",len(trigrams)/poss*100,\"\\n\")\n",
    "\n",
    "# Quadgrams MLE Prob\n",
    "quadgrams=n_gram_split(train_data,4)\n",
    "\n",
    "quadgram_dict=word_freq_descending(quadgrams)\n",
    "\n",
    "\n",
    "quadgram_MLE={}\n",
    "for key,value in quadgram_dict.items():\n",
    "    word_1, word_2, word_3, word_4=key.split(\" \")\n",
    "    quadgram_MLE[key]=value/trigram_dict.get(word_1+\" \"+word_2+\" \"+word_3)\n",
    "    \n",
    "print(\"Number of Actual Quadgrams: \",len(quadgrams))\n",
    "poss=int(N*(N-1)*(N-2)*(N-3))\n",
    "print(\"Number of Possible Quadrams: \",poss)\n",
    "print(\"Percentage of Quadgrams in the document: \",len(quadgrams)/poss*100,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(model_name):\n",
    "    if type(model_name)==dict:\n",
    "        sent=\"<s>\"\n",
    "        last_word=\"<s>\"\n",
    "        last_words=\"\"\n",
    "        while last_word!=\"</s>\":\n",
    "            n_gram=list(model_name.keys())\n",
    "            if len(n_gram[0].split(\" \"))>1:\n",
    "                n_gram=[gram for gram in n_gram if last_words in \" \".join(gram.split(\" \")[:len(gram.split(\" \"))-1])]\n",
    "                n_gram_prob=[model_name[x] for x in n_gram]\n",
    "                words=str(random.choices(n_gram,n_gram_prob)[0])\n",
    "                words_in_n_gram=words.split(\" \")\n",
    "\n",
    "                while last_word==words_in_n_gram[-1]:\n",
    "                    print(\"wait\")\n",
    "                    n_gram=list(model_name.keys())\n",
    "                    n_gram=[gram for gram in n_gram if last_words in gram]\n",
    "                    n_gram_prob=[model_name[x] for x in n_gram]\n",
    "                    words=str(random.choices(n_gram,n_gram_prob)[0])\n",
    "                    words_in_n_gram=words.split(\" \")\n",
    "            else:\n",
    "                n_gram.remove(\"<s>\")\n",
    "                n_gram_prob= [model_name[x] for x in n_gram]\n",
    "                words=str(random.choices(n_gram,n_gram_prob)[0])\n",
    "                words_in_n_gram=[words]\n",
    "\n",
    "            if len(words_in_n_gram)>1:\n",
    "                last_words=\" \".join(words_in_n_gram[1:len(words_in_n_gram)])\n",
    "                last_word=words_in_n_gram[-1]\n",
    "            else:\n",
    "                last_words=words\n",
    "                last_word=words\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(sent.split(\" \"))==1:\n",
    "                sent=sent+\" \"+last_words\n",
    "            else:\n",
    "                sent=sent +\" \"+last_word\n",
    "        return sent\n",
    "    \n",
    "    else:\n",
    "        print(\"Model must be a dictonary of n-gram probabilities\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Model Name= dictonary containing probility value\n",
    "#     unigram_MLE - for unigram\n",
    "#     bigram_MLE - for bigram\n",
    "#     bigram_add_1 - for bigram add1 smoothed\n",
    "#     trigram_MLE - for trigram\n",
    "#     quadgram_MLE - for quadgram\n",
    "\n",
    "# uncomment and provide model_name to use\n",
    "\n",
    "# print(generator(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sentence,model_name):\n",
    "    if type(sentence)!=type(\" \"):\n",
    "        print(\"Enter sentence\")\n",
    "        return None\n",
    "            \n",
    "    words=sentence.split(\" \")\n",
    "    words=list(filter(lambda x: x != \"\", words))\n",
    "    prob=0\n",
    "    N_grams=[]\n",
    "    if type(model_name)==dict:\n",
    "        model_gram=list(model_name.keys())\n",
    "        n=len(model_gram[0].split(\" \"))\n",
    "        \n",
    "        if len(words)<n:\n",
    "            print(\"Sentence too small to use on this specific model_name\")\n",
    "            return None\n",
    "        \n",
    "        if n==1:\n",
    "            N_grams=words\n",
    "            for gram in N_grams:\n",
    "                gram_prob=model_name.get(gram,0)\n",
    "                if gram_prob>0:\n",
    "                    prob=prob+math.log(gram_prob)\n",
    "            return prob\n",
    "\n",
    "        elif n==2:\n",
    "            for i in range(0,len(words)-(n-1)):\n",
    "                n_gram=[]\n",
    "                for j in range(0,n):\n",
    "                    n_gram.append(words[i+j])\n",
    "                N_grams.append(\" \".join(n_gram))\n",
    "            for gram in N_grams:\n",
    "                gram_prob=model_name.get(gram,0)\n",
    "                if gram_prob>0:\n",
    "                    prob=prob+math.log(gram_prob)\n",
    "            word_1=N_grams[0].split(\" \")[1]\n",
    "            word_1_uni=unigram_MLE.get(word_1,0)\n",
    "            if word_1_uni!=0:\n",
    "                word_1_prob=math.log(word_1_uni)\n",
    "            else:\n",
    "                word_1_prob=0\n",
    "            prob=prob+word_1_prob\n",
    "\n",
    "            return prob\n",
    "\n",
    "        elif n==3:\n",
    "            for i in range(0,len(words)-(n-1)):\n",
    "                n_gram=[]\n",
    "                for j in range(0,n):\n",
    "                    n_gram.append(words[i+j])\n",
    "                N_grams.append(\" \".join(n_gram))\n",
    "            for gram in N_grams:\n",
    "                gram_prob=model_name.get(gram,0)\n",
    "                if gram_prob>0:\n",
    "                    prob=prob+math.log(gram_prob)\n",
    "\n",
    "            word_1=N_grams[0].split(\" \")[1]\n",
    "            word_1_uni=unigram_MLE.get(word_1,0)\n",
    "            word_1_prob=math.log(word_1_uni)\n",
    "\n",
    "            word_1_2=\" \".join([N_grams[0].split(\" \")[1],N_grams[0].split(\" \")[2]])\n",
    "            word_1_2_bi=bigram_MLE.get(word_1_2,0)\n",
    "            word_1_2_prob=math.log(word_1_2_bi)\n",
    "\n",
    "            prob=prob+word_1_prob+word_1_2_prob\n",
    "\n",
    "            return prob\n",
    "\n",
    "        elif n==4:\n",
    "            for i in range(0,len(words)-(n-1)):\n",
    "                n_gram=[]\n",
    "                for j in range(0,n):\n",
    "                    n_gram.append(words[i+j])\n",
    "                N_grams.append(\" \".join(n_gram))\n",
    "            prob=0\n",
    "            for gram in N_grams:\n",
    "                gram_prob=model_name.get(gram,0)\n",
    "                if gram_prob>0:\n",
    "                    prob=prob+math.log(gram_prob)\n",
    "\n",
    "            word_1=N_grams[0].split(\" \")[1]\n",
    "            word_1_uni=unigram_MLE.get(word_1,0)\n",
    "            word_1_prob=math.log(word_1_uni)\n",
    "\n",
    "            word_1_2=\" \".join([N_grams[0].split(\" \")[1],N_grams[0].split(\" \")[2]])\n",
    "            word_1_2_bi=bigram_MLE.get(word_1_2,0)\n",
    "            word_1_2_prob=math.log(word_1_2_bi)\n",
    "\n",
    "            word_1_2_3=\" \".join([N_grams[0].split(\" \")[1],N_grams[0].split(\" \")[2],N_grams[0].split(\" \")[3]])\n",
    "            word_1_2_3_tri=trigram_MLE.get(word_1_2_3,0)\n",
    "            word_1_2_3_prob=math.log(word_1_2_3_tri)\n",
    "            prob=prob+word_1_prob+word_1_2_prob+word_1_2_3_prob\n",
    "\n",
    "            return prob\n",
    "        \n",
    "        else:\n",
    "            print(\"Cant process more than quadgram due to non-inclusion of chain rule probabilities\")\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        print(\"Model must be a dictonary of n-gram probabilities\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Name= dictonary containing probility value\n",
    "#     unigram_MLE - for unigram\n",
    "#     bigram_MLE - for bigram\n",
    "#     bigram_add_1 - for bigram add1 smoothed\n",
    "#     trigram_MLE - for trigram\n",
    "#     quadgram_MLE - for quadgram\n",
    "\n",
    "# uncomment and provide model_name and sentence to use\n",
    "\n",
    "#sentence=\"\"\n",
    "\n",
    "# print(probability(sentence,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible bigrams\n",
    "items=list(unigram_dict.keys())\n",
    "all_bigrams=list(itertools.permutations(items,2))\n",
    "\n",
    "all_bigrams=[\" \".join(x) for x in all_bigrams]\n",
    "\n",
    "# Add the multiple occurence of possible bigrams to the permutation\n",
    "all_bigrams.extend(bigrams)\n",
    "\n",
    "all_bigram_dict=word_freq_descending(all_bigrams)\n",
    "for key,value in all_bigram_dict.items():\n",
    "    all_bigram_dict[key]=value-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Add 1 Prob\n",
    "\n",
    "bigram_add_1={}\n",
    "for key,value in all_bigram_dict.items():\n",
    "    word_1, word_2=key.split(\" \")\n",
    "    bigram_add_1[key]=(value+1)/(unigram_dict.get(word_1)+V)\n",
    "\n",
    "\n",
    "diff_dict={}\n",
    "for key in bigram_add_1.keys():\n",
    "    diff_dict[key]=abs(bigram_MLE.get(key,0)-bigram_add_1[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples\n",
      "\n",
      "| Bigram      |   Prob before smoothing |   Prob after add1 smoothing |   Abs Difference |\n",
      "|-------------+-------------------------+-----------------------------+------------------|\n",
      "| the and     |                 0       |                 4.19463e-05 |      4.19463e-05 |\n",
      "| <s> lamps   |                 0       |                 4.27277e-05 |      4.27277e-05 |\n",
      "| and faintly |                 0       |                 4.307e-05   |      4.307e-05   |\n",
      "| said the    |                 0.45082 |                 0.00722116  |      0.443599    |\n",
      "| of the      |                 0.26817 |                 0.00469137  |      0.263479    |\n",
      "| said Alice  |                 0.26776 |                 0.00430659  |      0.263453    |\n",
      "\n",
      "This drastic change in effective count and probability before and after smoothing because in add 1 smoothing no weightage is given to more frequent occuring bigrams, and thus while distributing the probability density function to unseen bigram such drastic changes occur. \n",
      "In other words this smoothing technique takes away too much probability mass from seen events and assigns too much total probability mass to unseen events. For example \n",
      "Before smoothing 'the </s>' probability= 0\n",
      "After smoothing 'the </s>' probability= 4.194630872483221e-05 which is more than without smoothing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(\"Examples\\n\")\n",
    "table=[\n",
    "    ['the and',bigram_MLE.get(\"the and\",0),bigram_add_1[\"the and\"],diff_dict[\"the and\"]],\n",
    "    ['<s> lamps',bigram_MLE.get(\"<s> lamps\",0),bigram_add_1[\"<s> lamps\"],diff_dict[\"<s> lamps\"]],\n",
    "    ['and faintly',bigram_MLE.get(\"and faintly\",0),bigram_add_1[\"and faintly\"],diff_dict[\"and faintly\"]],\n",
    "    ['said the',bigram_MLE[\"said the\"],bigram_add_1[\"said the\"],diff_dict[\"said the\"]],\n",
    "    ['of the',bigram_MLE[\"of the\"],bigram_add_1[\"of the\"],diff_dict[\"of the\"]],\n",
    "    ['said Alice',bigram_MLE[\"said Alice\"],bigram_add_1[\"said Alice\"],diff_dict[\"said Alice\"]]\n",
    "      ]\n",
    "\n",
    "print (tabulate(table, headers=['Bigram','Prob before smoothing','Prob after add1 smoothing','Abs Difference'],tablefmt='orgtbl'))\n",
    "answer=\"This drastic change in effective count and probability before and after smoothing \\\n",
    "because in add 1 smoothing no weightage is given to more frequent occuring bigrams, and thus \\\n",
    "while distributing the probability density function to unseen bigram such drastic changes occur. \\\n",
    "\\nIn other words this smoothing technique takes away too much probability mass from seen events and \\\n",
    "assigns too much total probability mass to unseen events. For example \\nBefore smoothing \\'the </s>\\' probability= \"+str(bigram_MLE.get(\"the </s>\",0)) +\"\\n\" + \"After \\\n",
    "smoothing \\'the </s>\\' probability= \"+str(bigram_add_1[\"the </s>\"])+ \" which is more than without smoothing.\"\n",
    "\n",
    "print(\"\\n\"+answer+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvrs_bigram_dict=inverse_dict(all_bigram_dict)\n",
    "bigram_Nc={}\n",
    "for key, value in rvrs_bigram_dict.items():\n",
    "    bigram_Nc[key]=len(value)\n",
    "adj_bigram_count={}\n",
    "for i in range(10,0,-1):\n",
    "    adj_bigram_count[i]=bigram_Nc[i+1]*(i+1)/bigram_Nc[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAENCAYAAADnrmWtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VNX9//HXmYQkQCAhGyFhEwER0aAoBAFFCC4sRRGiCCqgogRF0FqotqJYbRT52mqx2A03FGVLAMEl7KAVFISI7LJHyAYhhCSTmXt+f0zJjwgJlyQzdybzeT4efTyYO8t5k+I7M3fOPUdprTVCCCH8is3qAEIIITxPyl8IIfyQlL8QQvghKX8hhPBDUv5CCOGHpPyFEMIPSfkLIYQfkvIXQgg/JOUvhBB+SMpfCCH8UKDVAaqSlZVVredFRUWRm5tby2ncx5fySlb38aW8ktV9apI3Li7O9GPlnb8QQvghKX8hhPBDHjvts3TpUlauXIlSihYtWpCSkkJQUJCnhhdCCHEOj7zzz8/PZ/ny5aSmpjJjxgwMw+Drr7/2xNBCCCEuwGOnfQzDwG6343Q6sdvtNGnSxFNDCyGE+BWPnPaJiIhg0KBBjBs3jqCgIBISEkhISPDE0EIIIS5AeWInr9OnTzNjxgwmTZpEgwYN+L//+z8SExO56aabKjwuIyODjIwMAFJTU7Hb7dUaLzAwEIfDUePcnuJLeSWr+/hSXl/I6jiWRdHH/8A4kYutSRQNh48lMNb8VEir1ORneynfo3rknX9mZiYxMTE0btwYgG7durF79+7zyj8pKYmkpKTy29Wd6+pP83o9TbK6jy/l9fasRs4x9BvPQ86x8mMlO7ahJk3DFh1rYbKLq1Pz/KOiotizZw+lpaVorcnMzCQ+Pt4TQwsh/FH6nArFD7hup8+xJo8X8sg7/3bt2pGYmMjkyZMJCAigdevWFd7hCyFEbdIn8y/puD/y2Dz/5ORkkpOTPTWcEMKPqfAILvRlpgqP8HgWbyVX+Aoh6p7BI+DX5/ajY13HBeDlC7sJIUR12KJjMSZNg/Q5BBYV4mjYCAaP8Povez1Jyl8IUSfZomPh4aeJ8PKZSVaR0z5CCOGHpPyFEMIPSfkLIYQfkvIXQgg/JOUvhBB+SMpfCCH8kJS/EEL4ISl/IYTwQ1L+Qgjhh6T8hRDCD0n5CyGEH5LyF0IIPyTlL4QQfkjKXwgh/JCUvxBCeAFtODHWf0XZwX0eGU/W8xdCCAtpreHHzRgL3oWjByk5cS8Mus/t40r5CyGERfTBfRjzZ8PObRAdi+2xyYTe+htK8/LcPrZHyj8rK4s33nij/HZ2djbJyckMGDDAE8MLIYRX0XnZ6EUfoL9dA6GNUPeORd18GyqwHkopj2TwSPnHxcUxffp0AAzD4NFHH6Vr166eGFoIIbyGLjqNXjYPvXIJKBvqjqGo2+9GNWjo8SweP+2TmZlJbGws0dHRnh5aCCEsocvK0Ks+Q3/2KRQXobr3QQ0egYqIsiyTx8t/w4YN9OjRw9PDCiGEx2nDQG9ah170AeRlw1XXYrt7FKrFZVZHQ2mttacGczgcPProo8yYMYPw8PDz7s/IyCAjIwOA1NRU7HZ7tcYJDAzE4XDUKKsn+VJeyeo+vpRXsl6c/cctFL73Fo69Owm8rB2hD4wnuPPFT3fXJG9QUJDpx3q0/Ddt2sQXX3zBH/7wB1OPz8rKqtY4UVFR5ObmVuu5VvClvJLVfXwpr2StnM46hLHgPdi2CSKiUINHohJ7o2zmLquqSd64uDjTj/XoaR855SOEqKv0yXz04o/Q6zMgJAQ15EFU34GooGCro12Qx8q/tLSUbdu2MXbsWE8NKYQQbqdLzqC/SEN/uQicTlSfAagB96AaNbY6WpU8Vv7BwcH85z//8dRwQgjhVtrpRK/7Er3kYzh1EnV9T9Rd96NimlkdzRS5wlcIIS6B1hq2fus6r3/sKLTriG38c6g2V1gd7ZJI+QshhEn6512u5Rj2/ASxzbGNfw4SunrsqtzaJOUvhBAXobN/cS3H8N16aByOGpmC6tkPFRBgdbRqk/IXQohK6MJT6M8+Qa9eDgEBqEH3om69ExXSwOpoNSblL4QQv6LtpegVS9HL50NJMapXP9Sg4ajwCKuj1RopfyGE+B9tGOj/rkanfwj5uZDQFduQB1BxLa2OVuuk/IUQAtA/bcGY9y4c2Q+t2mIbMwl1xdVWx3IbKX8hhF/Th/e7dtHavgWimqIe+a1rzr7J5Rh8lZS/EMIv6fxcdNqH6P+ugvoNUckPoXr3R9WrZ3U0j5DyF0L4FX2mCP35AnTGYtDaNXvnjmGohqFWR/MoKX8hhF/QjjL0ms/RS+fC6UJU4i2oO0egImOsjmYJKX8hRJ2mtUZ/vwFj4fuQ/QtcmYBt6ChUy8utjmYpKX8hRJ2l9/zEiekfYOzeDvGtsD05Fa66zieXY6htUv5CiDpHHzuCseB9+OG/rg1VRk1Adb8FZfPd5Rhqm5S/EMI0I+cYpM8hv6gQo2EjGDwCW3Ss1bHK6VMn0Evmotd+AUHBqDtHEnXvGPIKT1sdzetI+QshTDFyjqHfeB5yjlF29uDPuzAmTbP8F4AuLUF/lYb+fBE47Kib70ANvAfVOBwVHAJS/ueR8hdCmJM+B3KOVTz2v08CPPy0JZG04URvWIFO/wgK8uG6G7HddT8qNt6SPL5Eyl8IYYo+mX9Jx91Jaw2Z37k2VMk6BJd3wPbYZFTbKz2exVdJ+QshTFHhEehKjnuSPrgXY95s2JUJMXHYxk2Ba7vLDJ5LJOUvhDBn8Aj4eVfFUz/Rsa7jHqBzj6MXfYjeuAYahaHuexTV6zZUoNRYdXjsp1ZUVMSsWbM4fPgwSinGjRtH+/btPTW8EKKGbNGxGJOmQfocAosKcXhoto8uKkQvm4deuRRsNlT/ZNTtQ1D1fX9DFSt5rPxnz55N586defrpp3E4HJSWlnpqaCFELbFFx8LDTxMRFUVubq5bx9JlZehVS9GfzYPiItSNfVGDR6CaRLp1XH9hqvz37NlDu3btzju+d+9e2rZte9Hnnzlzhh07djB+/HjXoIGBBMpHNSHEBWjDQG9ah170AeRlQ6cu2O5+ENW8tdXR6hSltb7QdzgVPPjgg7z33nvnHR89ejSzZ8++6CAHDhzgnXfeoXnz5hw8eJA2bdowatQoQkJCKjwuIyODjIwMAFJTU7Hb7Wb/HhUEBgbicDiq9Vwr+FJeyeo+vpTXXVntmd9T+N5MHPt2EtimPaEPPk7wNdfX6DV96ecKNcsbFBRkfpyq7jQMA/jfwkj/+99Zx48fJ8DkzvVOp5P9+/czZswY2rVrx+zZs0lLS+Pee++t8LikpCSSkpLKb1f3Y2WUBz6S1iZfyitZ3ceX8tZ2Vn30kGtDlczvICIa9dAkjK43U2izUVjDcXzp5wo1yxsXF2f6sVWW//Dhw8v//Ouittls3HXXXaYGiYyMJDIysvzUUWJiImlpaaZDCiHqJn0yD53+EXrDCgipjxo6CtVnIKqe+XewonqqLP+//e1vaK154YUXePHFF8uPK6Vo3Lix6Y8Y4eHhREZGkpWVRVxcHJmZmTRv3rxmyYUQPkuXnEF/vhD9VRo4DVTfQagBw1Chja2O5jeqLP/o6GgA3n777RoPNGbMGN58800cDgcxMTGkpKTU+DWFEL5FOxzodV+il3wMhQWoG3qh7rof5UWLw/kLU1NuTp8+zeLFizl48CAlJSUV7jv3E0FVWrduTWpq6qUnFEL4PK01/PCtazmG40ehfSdsTzyPuuz8WYTCM0yV/1//+lccDgfdu3e/pG+ThRBC79uJMf9d2PsTNGuB7fE/wjXXy3IMFjNV/rt37+Zf//oX9fxkV3shRM3p7CzX1onffw1hTVD3j0f1SEKZnCUo3MtU+bds2ZK8vDxiY+W8nBCiarqwAL30E/Sa5RBYD/Wb+1D9BqNC6lsdTZzDVPl36tSJV155hd69exMeHl7hvj59+rglmBDCt2h7KTpjMfrzBVBagup1K2rQcFRYE6uj+QRP75Jmqvx37txJZGQkmZmZ590n5S+Ef9OGE/3NanT6HDiRCwldXcsxNGthdTSfYcUuaabKf+rUqW4ZXAjh2/SPm11X5h45AJe1x/bwU6j2nayO5Xss2CXNVPmfXebhQmw2W62FEUL4hrL9u3H+6y/w0w8QHYsa+zvU9T1kBk81WbFLmqnyP3eZh1/75JNPai2MEMK76bwcdPqH5P93NTQIRd3zsGuzdJkJWCNW7JJmqvz/9re/Vbh94sQJ0tLSuP76mq22J4TwDfrMafSy+egVSwBocOd9lPQegGoQanGyOsKCXdJMlf/ZZR7Ovf3444/z+9//Xr7wFaIO044y9Orl6M8+gaLTqMTeqMEjaXTFlZT60EqZ3s6KXdKqvaPKmTNnOHXqVG1mEUJ4Ca01+rsN6EXvu96NXpmAbegoVMvLrY5WZ3lylzQwWf5vvfVWhS9ySktL2bFjB7169XJbMCGENfTu7RjzZ8P+3RDfCtuTL6A6XWd1LFHLTJX/r6/sDQ4Opl+/flxzzTVuCSWE8Dz9yxHXtM2tGyE8EjXqSVT33iibLMdQF5kq/2HDhrk7hxDCIrrgBHrxx+j1X0JQsGuJ5b6/QQUHWx1NuJHpc/6rVq1i7dq15OfnExERwU033cQtt9zizmxCCDfSJcXoL9PQXy4CRxmqd3/UwHtQjcKsjiY8wFT5L1y4kDVr1jBo0KDy/SUXL17MiRMnGDJkiLszCiFqkXY60Ru+Qi/+GApOQJcbsd31AKqp+f1fhe8zVf4rVqzghRdeqDDlMyEhgalTp0r5C+EjtNawbZNrQ5VfDkPbK7GN+z3q8g5WRxMWMFX+paWlNG5ccW/NRo0aYbfb3RJKCFG79IE9GPNmw+4foWk8tnG/h2sTZTkGP2aq/Dt37sybb77JiBEjiIqKIicnh48//piEhAR35xNC1IDOOYZe9AF60zpoFIa67zHXUsuB1b7ER9QRpv4FjBkzhv/85z8888wzOBwOAgMDSUxMZMyYMaYHGj9+PCEhIdhsNgICAmQ/XyHcSBcVopd+il71GQTYUAOSUbcNQdVvYHU04SVMlX+DBg14/PHHSUlJobCwkEaNGlVrNc+pU6eed/pICFF7dJkdvXIpetk8KC5G9eiLGnwfKjzS6mjCy1RZ/ocPH2bz5s0MHjwYcC3fHBbmmgaWnp5Oly5daN68uftTCiGqpA0DvXENetGHkJ8DV1/v2lAlvpXV0YSXqrL858+fzw033HDB+6Kjo5k/fz4TJ040PdjLL78MQL9+/UhKSrqEmEKIyugdW13LMRz6GVpejm3UBNSV8n2cqJrSWl9oGWkAxo0bx1//+leCgoLOu6+srIwJEybw97//3dRAZy8OKygo4E9/+hOjR4+mY8eOFR6TkZFBRkYGAKmpqdWeTRQYGIjD4ajWc63gS3klq/tcat6yg/s4/f7b2Dd/gy06ltCRjxHSMwnlgQ2WfOln60tZoWZ5L9TVlY5T1Z2nT5+u9Ny+UorTp0+bHigiwrUpQVhYGDfccAN79+49r/yTkpIqfCKo7sp2UR5aFa+2+FJeyeo+ZvPqE3no9Dnor1dC/fqooaOhzwCK6gVRlO++nZ/O5Us/W1/KCjXLGxdn/kK9Kss/JiaG3bt3n1fSALt37yYmJsbUICUlJWitqV+/PiUlJWzbto2hQ4eaDimEAF18Bv35QnRGGhgGKmmQaxZPw0ZWRxM+qMry79u3L7NmzWLixIm0adOm/PjPP//MO++8w+23325qkIKCAl5//XUAnE4nPXv2pHPnzjWILYT/0A4Het0X6CVzobAA1fUm1J0jUW7c6EPUfVWWf//+/Tl27BjPPvsskZGRNGnShBMnTpCfn8+tt97KHXfcYWqQpk2bMn369FoJLIS/0FrDlm8wFrwP2VlwxdWuDVVat7M6mqgDLjrPf8yYMfTv35/MzMzyOf5XX331eWv8CyFqj967wzWDZ99OaNYC2xN/hKuvl+UYRK0xvZmLlL0Q7qePZ2EsfA82fwNhEagHHkfd2BcVIBuqiNolC3wI4QV0YQGnFr2P8cUiCAxyXZXb705UcIjV0UQdJeUvhIV0aSk6Ix39+QKK7XbUTbeiBt2LatzE6miijpPyF8IC2nCiv16JTp8DJ/OhczciH3qSkyGhVkcTfkLKXwgP0lrD9s0Y89+FowfhsvbYHnkG1f4qAqOiwIcuRhK+rdLyHzdunKkXMLu8gxD+Th/ch7HgXdixFaJjsT36O+jSQ2bwCEtUWv5PPPFE+Z/37t3LmjVruOOOO4iOjiYnJ4cvvviCm266ySMhhfBlOi8bnfYh+r+rIbQR6t5HUDffjgqsZ3U04ccqLf9zl3T497//zXPPPVe+Pg/AtddeyyuvvMKgQYPcm1AIH6WLTqOXz0OvWApKoe64G3X73agGcl5fWM/UOf/8/HxCQipOOQsJCSHfQ4tICeFLdFkZevUy9GefwpnTqMRbUHeOQEVEWx1NiHKmyv/666/n1Vdf5e677yYiIoK8vDzS0tLo0qWLu/MJ4TO0YaC/W49e9AHkHoeO17o2VGnZ5uJPFsLDTJX/I488wrx58/jnP/9Zvi5/YmIiw4YNc3c+IXyC3vWjazmGA3ugeWtsE19EXXWt1bGEqJSp8g8KCmLEiBGMGDHC3XmE8Ck66xDGwvdh60ZoEoUa/SQqsTfKJssxCO9mep7/tm3b2LBhAwUFBUyZMoV9+/ZRXFxMp06d3JlPCK+kT+ajF3+EXp8BISGoIQ+g+g5CBQVbHU0IU0zt97Z8+XL++c9/0qxZM3bs2AG4Pg3MnTvXreGE8Da6pBhj8UcYf3gM/fUKVJ8B2F7+B7Y7hkrxC59i6p3/smXL+OMf/0hMTAzp6ekAxMfHk5WV5dZwQngL7XSi13+FXvwRnDqJ6tIDNeR+VIz5bfMqY+Qcg/Q55BcVYjRsBINHYJONWoSbmSr/4uJioqKiKhxzOBwEBsrqEKJu01rD1o0YC96DY0egbUdsKc+iLu9QK69v5BxDv/E85Byj7OzBn3dhTJomvwCEW5k67XPllVeSlpZW4djy5cu56qqr3BJKCG+g9+/GeP1ZjJkvAxrb+Gex/e7PtVb8AKTPgZxjFY/975OAEO5k6q37mDFjePXVV1mxYgUlJSU8+eSTNGjQgMmTJ7s7nxAep7N/QS/6AP3demgUhhoxDtWzH8oNn3T1yQtfKFnZcSFqi6l/zU2aNOHPf/4z+/btIycnh8jISNq2bYvNZuqDg6gD/OG8tD59Cr30E/Tq5RAQgBp4L+q2O1EhDdw2pgqPQFdyXAh3MtXer732Gkop2rZtS/fu3Wnfvj02m43XX3/9kgYzDIPf/e53pKamViussMbZ89L62zWU/bgZ/e0a9BvPu34h1AHaXoqxfAHGs4+iV36GurEPtpdnYRt8n1uLH4DBI+DXv0SjY13HhXAjU+/8t2/ffknHK7Ns2TLi4+MpLi6+pOcJi1V1Xvrhp63JVAu0YaD/uxqd/iHk58I1N2Ab8iAqvqXHMtiiYzEmTYP0OQQWFeKoo5+qhPepsvw/+eQTwDWz5+yfzzp+/DjR0eYXqsrLy2Pz5s0MGTKEpUuXViOqsEpdPC+tf9ri2lDl8H5o1Rbb6ImoDtdYksUWHQsPP01EVBS5spmL8JAqyz8vLw9wna45++ezoqKiSE5ONj3Qu+++y8iRI+Vdvw+qS+el9ZH9rtLfvgUiY1APP426oRdKvr8SfqbK8k9JSQGgffv2JCUlVXuQ77//nrCwMNq0aVPlqaKMjAwyMjIASE1NPe/aArMCAwOr/VwreHtex6gnOHlgL87jR8uPBTSNJ3zUE66tB73UuT9XZ242pz/+ByWrlqMahtJw1BM06H83ql6QxSn/P2//d3Auyeo+nsqrtNYXelNXwZEjRwgNDSU8PJySkhIWL16MzWZj0KBBBAdf/JL2jz76iLVr1xIQEIDdbqe4uJiuXbsyYcKEKp9X3SuIo3zs47Mv5D0728eXzktHRUWRc+gg+vMF6IzFoA1Un0Go/sNQDb1vQxVf+HdwlmR1n5rkjYszf8W5qfJ/5plnmDRpEnFxcfzjH//gl19+oV69ejRq1KjCdo9mbN++nSVLljBlypSLPlbK3/v4SlbtKKPh9+spnPtvOH0K1e1m1J0jUVFNrY5WKV/52YJkdSdPlb+p2T45OTnExcWhtWbTpk3MmDGDoKAgHn/88WoFFMJdtNaw+WuMhe9TmP0LXHE1tmGjUa3aWh1NCK9iqvzr1atHcXExR44cITIyksaNG+N0OikrK7v4k3/lqquukmUhhFvovT9hzJsNP++CuJaE/+F1TrVsh1LK6mhCeB1T5d+jRw+mTZtGcXExt99+OwD79+8nJibGreGEMEMfO+LaUGXLfyEsAvXA46gb+xLctCnKhz7uC+FJpsp/1KhRbN26lYCAgPLNW5RSPPjgg24NJ0RV9KkT6CVz0Wu/gHrBqMEjUP0Go4JDrI4mhNczvVJVQkJChduXX355rYcRwgxdWoL+Kg39+SIoK0XddDtq0L2oxuFWRxPCZ5gq/+eff77S86YvvvhirQYSojLacKI3rECnfwQF+XBtIrYhD6Bim1sdTQifY6r8+/TpU+H2yZMnWbVqFb169XJLKCHOpbWGH793XZmbdQjaXIHt0d+h2nW0OpoQPstU+ffu3fu8Y4mJibz99tsMHTq0tjMJUU4f3OuawbMrE2KaYXtsMlx3o8zgEaKGqr07RUREBAcPHqzNLEKU07nH0Ys+RG9cA6GNUfeORd18Gyqw3kWf6w97DwhRU6bKf+XKlRVu2+12vv32W9q3b++WUMJ/6aJC9LJ56JVLQdlcSzHcNgTVoKGp58ueuEKYY6r8161bV+F2cHAwV1xxBQMGDHBLKOF/dFkZetVS9GfzoLgI1b2Pa+pmxCUucFVH9x4QoraZKv+pU6e6O4fwU9ow0JvWoRd9AHnZ0Ok6bHc/iGp+WfVerw7uPSCEO1Ra/tnZ2eVX8B4/frzyFwgMJDw8nICAgNpPJ+o0vXObawbPwb3Q4jJsD0xDdexco9esS3sPCOFOlZb/b3/7W95//32Aiy69HBQUxIgRI8qXfhCiKvroIYwF70LmdxARhRozybXqZm1sqDJ4hGttn3NP/cieuEKcp9LyP1v8wHlbOJ5La82hQ4eYNm2alL+okj6Zh178MXp9BoTUR939IKrPQFTQxfeEMEv2xBXCnGpP9TxLKUWrVq0YO3ZsbeQRdZAuOYP+YhH6yzRwOlF9B6IGJKNCG7tlPNkTV4iLq7T8q1rS4Vxnl3fo1q1b7aUSdYJ2ONDrv0Qv/hgKC1x75d45EhXTzOpoQvi9Ssv/3CUdjh8/zqpVq7j55puJjo4mNzeXNWvWcMstt3gkpPAtWmv44VuMBe/B8aPQriO2J/6IukyuCxHCW1Ra/ucu6fDcc8/x3HPP0aJFi/JjPXv25O9//zvJycluDSh8i9630zWDZ+9PENsc2/jnIKGrLMcghJcxdc7/yJEjNG1ace/TmJgYjh496pZQwvfo7CzXhirffw2Nw1EjU1A9+6FkCrAQXslU+Xfs2JG3336be+65h8jISHJzc5k3bx4dOnRwdz7h5XThKfTSueg1yyEg0LWu/q13oULqWx1NCFEFU+U/fvx4/vWvf/HUU09hGAYBAQF07dqVlJQUd+cTXkrbS9EZi9GfL4CSElSvfqhBw+ViKiF8hKnyDw0NZeLEiRiGwalTp2jcuDE2mw3DMEwNYrfbmTp1Kg6HA6fTSWJionxX4KO004mxYQU6fQ6cyIWErq7lGJq1uPiThRBe45Lm+dtsNsLDwzl06BBr1qxh/fr1vPPOOxd9Xr169Zg6dSohISE4HA6ef/55OnfuLKuC+hj942by0z9EH9gLrdthe+gp1BWdrI4lhKgG0+V/6tQp1q9fz5o1azhw4AAdOnRg1KhRpp6rlCIkxLWpttPpxOl0yuwPH6IP/exajuGnH1BN41Bjn0F16VE7yzEIISxRZfk7HA6+++47Vq9ezdatW4mNjaVHjx7k5OTw1FNPERYWZnogwzCYPHkyx44d47bbbqNdu3Y1Di/cS+floNM/RP93NdRviEp+iMih95NXcMrqaEKIGlJa6wstggjA6NGjsdls3HzzzfTs2ZM2bdoAMHbsWKZPn35J5X9WUVERr7/+OqNHj6Zly5YV7svIyCAjIwOA1NRU7Hb7Jb8+uFYadTgc1XquFbwtr1FUSNGCDziz9FMAGgwcRsMh92MLbex1WaviS1nBt/JKVvepSd6goCDz41R1Z6tWrdi5cyd79+6lWbNmxMTEEBoaWq1QZzVs2JCOHTvyww8/nFf+SUlJJCUlld+u7rosUT62pou35NWOMvTq5ejPPoHThajEW1B3jqA0MobSEjuU5HpNVjN8KSv4Vl7J6j41yRsXF2f6sVWW/wsvvEBOTg5r1qxhyZIlzJ49m2uuuYbS0lKcTqfpQU6dOkVAQAANGzbEbreTmZnJ4MGDTT9fuJfWGv3dBvSi911LIV+ZgG3oKFTLy62OJoRwk4t+4RsdHc3QoUMZOnQoO3fuZM2aNSileOaZZ7jlllsYOXLkRQc5ceIEM2fOxDAMtNZ0796dLl261MpfQNSM3r0dY/5s2L8b4lthe3IqXHWdfCEvRB13SVM9O3ToQIcOHRg9ejQbN25k7dq1pp7XqlUrXnvttWoFFO6hfznimsGzdSOER6BGTUB1vwVlk+UYhPAH1VrPPygoiJ49e9KzZ8/aziPcTBecQC/5GL3uSwgKdi2xnDQYFVx7G6oIIbxfjTdzEb5BlxSjv0xDf7kIHGWom+9wrcPT6NJnbAkhfJ+Ufx2nnU70hgz04o+g4ARcdyO2u+5HxcZbHU0IYSEp/zpKaw3bvnOd1//lMFzeAdtjU1Btr7Q6mhDCC0j510H6wB6MebNh948QE4dt3BS4trvM4BFClJPyr0N0zjH0og/Qm9ZBozDUfY+iet2GCpT/m4UQFUkr1AG6qBC99FP0qs8gwIbqn4y6fQiqfgOrowkhvJSUvw/TZXb0yqU+T9NSAAAOk0lEQVToZfOg+Azqxr6owSNQTSKtjiaE8HJS/j5IGwZ64xr0og8hPwc6dXFtqNK8tdXRhBA+Qsrfx+gdW13LMRz6GVq2wTZqAurKBKtjCSF8jJS/j9BHD2LMfxd+/B4iolEPTUJ1vVk2VBFCVIuUv5fTJ/LQ6XPQX6+E+vVRQ0ej+gxA1TO/brcQQvyalL+X0sVn0J8vRGekgdNA9R2EGjAMFdrY6mhCiDpAyt/LaIcDve4L9JK5UFiA6nqTa/G16Firowkh6hApfy+htYYt32AseB+ys6B9J2xPPI+6TPY6FkLUPil/L2DfmYnxrzdg305o1gLb43+Ea66X5RiEEG4j5W8hfTwLY+H7nNj8NYQ1Qd0/HtUjCRUgG6oIIdxLyt8CurAAvWQueu3nEFiPhvc+THGPfqiQ+lZHE0L4CSl/D9KlpeiMdPTnC8Beiup1K2rQcEIvb0dJbq7V8YQQfkTK3wO04UR/swqdNgdO5kHnbtiGPIhq1tzqaEIIP+WR8s/NzWXmzJmcPHkSpRRJSUn079/fE0NbSmsN2ze7rsw9ehAua4/tkadR7TtZHU0I4ec8Uv4BAQHcf//9tGnThuLiYqZMmcI111xD8+Z1952vPrTPVfo7tkJ0LGrs71DX95AZPEIIr+CR8m/SpAlNmjQBoH79+sTHx5Ofn18ny1/nZaPT5qC/XQ0NQlH3PIzqfQcqsJ7V0YQQopzHz/lnZ2ezf/9+2rZt6+mh3UqfOY1eNg+9YikA6rYhqDvuRjUItTiZEEKcT2mttacGKykpYerUqQwZMoRu3bqdd39GRgYZGRkApKamYrfbqzVOYGAgDoejRlnN0mV2zny+iKJPZ6OLCgm5+XZC73uEgEtYjsGTeWtKsrqPL+WVrO5Tk7xBQeYXfPRY+TscDl599VUSEhIYOHCgqedkZWVVa6yoqChy3Tx1UmuN3rQOvegDyD0OHTtju3sUqmWbS34tT+StLZLVfXwpr2R1n5rkjYuLM/1Yj5z20Voza9Ys4uPjTRe/N9O7f8SYNxsO7IHmrbFNfBF11bVWxxJCCNM8Uv67du1i7dq1tGzZkmeeeQaA4cOHc91113li+FqjfzmMseA92LoRmkShRj+JSuyNslVvOQYj5xikzyG/qBCjYSMYPAKbrN4phPAAj5R/hw4d+PTTTz0xlFvoghPoxR+h130FISGoIQ+41tcPCq72axo5x9BvPA85xyg7e/DnXRiTpskvACGE28kVvlXQJcXoLxehv0wDR5lrB60ByahGYTV/8fQ5kHOs4rH/fRLg4adr/vpCCFEFKf8L0E4nev1X6MUfwamTqC49UEPuR8WY/zLlomOczL+k40IIUZuk/M+htYatG13n9Y8dgbZXYkt5FnV5h1ofS4VHcKFpVio8otbHEkKIX5Py/x+9fw/G/P/A7u3QNB5byrPQuZv7lmMYPAJ+3lXx1E90rOu4EEK4md+Xv845hl70AXrTOmgUhhrxGKrnrahA9/5obNGxGJOmQfocAosKcchsHyGEB/lt+evTp9CffYpetQwCAlAD70HddhcqpIHHMtiiY+Hhp4nwsYtQhBC+z+/KX9tL0SuXopfNh5JiVM8k1G+Go8IjrY4mhBAe4zflrw0D/e0adNoHkJ8LV1/vWo4hvqXV0YQQwuP8ovz1Tz9gzJ8Nh/dDq7bYRk9EdbjG6lhCCGGZOl3++sh+17TNHzdDZAzq4adRN/RC2WxWRxNCCEvVyfJ35mZjvPsW+uuVUL8hatgY1C0DUPVkQxUhhIA6WP7GV+nkpn0IhhPV705U/2GohrKhihBCnKvOlT8NQglJvBn7HcNQUU2tTiOEEF6pzpW/rUdfwgbfI/PmhRCiCvLNpxBC+CEpfyGE8ENS/kII4Yek/IUQwg9J+QshhB+qU7N9ZEN0IYQwxyPl//bbb7N582bCwsKYMWOGW8aQDdGFEMI8j5z26d27N88++6x7B6lqQ3QhhBAVeKT8O3bsSGioe5dYkA3RhRDCPK8655+RkUFGRgYAqampREVFmX5uQdNmlOzKPO94SNNmhF3C61ghMDDwkv6uVpKs7uNLeSWr+3gqr1eVf1JSEklJSeW3L2WJBuP2obBj23kbopfePtTrl3qI8qFtHCWr+/hSXsnqPjXJGxcXZ/qxXlX+NSEbogshhHl1pvxBNkQXQgizPFL+f/nLX/jpp58oLCzkscceIzk5mT59+nhiaCGEEBfgkfKfOHGiJ4YRQghhkizvIIQQfkjKXwgh/JCUvxBC+CGltdZWhxBCCOFZdfKd/5QpU6yOcEl8Ka9kdR9fyitZ3cdTeetk+QshhKialL8QQvihgBdeeOEFq0O4Q5s2bayOcEl8Ka9kdR9fyitZ3ccTeeULXyGE8ENy2kcIIfxQnVrYLTc3l5kzZ3Ly5EmUUiQlJdG/f3+rY12Q3W5n6tSpOBwOnE4niYmJJCcnWx2rSoZhMGXKFCIiIrx+BsX48eMJCQnBZrMREBBAamqq1ZEqVVRUxKxZszh8+DBKKcaNG0f79u2tjnVBWVlZvPHGG+W3s7OzSU5OZsCAARamqtzSpUtZuXIlSilatGhBSkoKQUFBVse6oGXLlrFixQq01vTt29f9P1Ndh+Tn5+t9+/ZprbU+c+aMnjBhgj58+LDFqS7MMAxdXFystda6rKxM//73v9e7du2yOFXVlixZov/yl7/oP//5z1ZHuaiUlBRdUFBgdQxT3nrrLZ2RkaG1dv1bOH36tMWJzHE6nfrhhx/W2dnZVke5oLy8PJ2SkqJLS0u11lrPmDFDr1q1ytpQlTh48KB+6qmndElJiXY4HHratGk6KyvLrWPWqdM+TZo0Kf+ipH79+sTHx5Of753bOCqlCAkJAcDpdOJ0OlFKWZyqcnl5eWzevJm+fftaHaVOOXPmDDt27Chf5TYwMJCGDRtanMqczMxMYmNjiY6OtjpKpQzDwG6343Q6sdvtNGnSxOpIF3T06FHatWtHcHAwAQEBXHnllWzcuNGtY9ap0z7nys7OZv/+/bRt29bqKJUyDIPJkydz7NgxbrvtNtq1a2d1pEq9++67jBw5kuLiYqujmPbyyy8D0K9fvwo7xHmT7OxsGjduzNtvv83Bgwdp06YNo0aNKn9j4M02bNhAjx49rI5RqYiICAYNGsS4ceMICgoiISGBhIQEq2NdUIsWLZg7dy6FhYUEBQWxZcsWLr/8creOWafe+Z9VUlLCjBkzGDVqFA0aNLA6TqVsNhvTp09n1qxZ7Nu3j0OHDlkd6YK+//57wsLCfGq63EsvvcSrr77Ks88+yxdffMFPP/1kdaQLcjqd7N+/n1tvvZXXXnuN4OBg0tLSrI51UQ6Hg++//57ExESro1Tq9OnTbNq0iZkzZ/LOO+9QUlLC2rVrrY51Qc2bN2fw4MH86U9/4pVXXqFVq1bYbO6t5zr3zt/hcDBjxgx69epFt27drI5jSsOGDenYsSM//PADLVu2tDrOeXbt2sV3333Hli1bsNvtFBcX8+abbzJhwgSro1UqIiICgLCwMG644Qb27t1Lx44dLU51vsjISCIjI8s/9SUmJvpE+W/ZsoXLLruM8PBwq6NUKjMzk5iYGBo3bgxAt27d2L17NzfddJPFyS6sT58+5af/PvroIyIjI906Xp1656+1ZtasWcTHxzNw4ECr41Tp1KlTFBUVAa6ZP5mZmcTHx1uc6sLuu+8+Zs2axcyZM5k4cSKdOnXy6uIvKSkpPz1VUlLCtm3bvPKXKkB4eDiRkZFkZWUBrsJq3ry5xakuzttP+YBrI/Q9e/ZQWlqK1tqr/xsDKCgoAFyzFjdu3Oj2n2+deue/a9cu1q5dS8uWLXnmmWcAGD58ONddd53Fyc534sQJZs6ciWEYaK3p3r07Xbp0sTpWnVBQUMDrr78OuE6r9OzZk86dO1ucqnJjxozhzTffxOFwEBMTQ0pKitWRqlRaWsq2bdsYO3as1VGq1K5dOxITE5k8eTIBAQG0bt3aa7/7AZgxYwaFhYUEBgby0EMPERoa6tbx5ApfIYTwQ3XqtI8QQghzpPyFEMIPSfkLIYQfkvIXQgg/JOUvhBB+SMpfCCH8UJ2a5y+EWevXr2fp0qUcPXqU+vXr07p1a4YMGUKHDh3cNmZycjJvvvkmsbGxbhtDCLOk/IXfWbp0KWlpaTzyyCMkJCQQGBjIDz/8wKZNm9xa/kJ4E7nIS/iVM2fO8Oijj5KSkkL37t3Pu7+srIw5c+bwzTffANC9e3dGjBhBvXr1WL16NStWrOCll14qf/y57+ZnzpxJcHAwOTk57Nixg+bNmzNhwgRiY2OZOnUqO3bsIDg4GIBx48Zx4403euYvLcQFyDl/4Vd2795NWVkZXbt2veD9CxcuZM+ePbz22mtMnz6dvXv3smDBAtOvv2HDBoYNG8bs2bOJjY1l7ty5ALz44osATJ8+nQ8++ECKX1hOyl/4lcLCQho1akRAQMAF71+/fj133303YWFhNG7cmKFDh7Ju3TrTr9+tWzfatm1LQEAAPXv25MCBA7WUXIjaJeUv/EqjRo0oLCzE6XRe8P78/PwKO1NFR0df0m5w5y5xHBwcTElJSfXDCuFGUv7Cr7Rv35569eqxadOmC94fERFBTk5O+e3c3NzyvQGCg4Ox2+3l9508edK9YYVwI5ntI/xKgwYNuOeee/j3v/+NzWYjISGBgIAAMjMz2b59Oz169GDhwoXl23/Onz+fXr16AdCqVSsOHz7MgQMHiIuL49NPP72kscPCwjh+/LhM9RReQcpf+J2BAwcSFhbGwoULeeuttwgJCaFNmzYMGTKEyy67jDNnzvDb3/4WcO2sNWTIEADi4uIYOnQoL730EkFBQQwfPpyMjAzT4w4bNoyZM2dit9sZO3asfOkrLCVTPYUQwg/JOX8hhPBDUv5CCOGHpPyFEMIPSfkLIYQfkvIXQgg/JOUvhBB+SMpfCCH8kJS/EEL4ISl/IYTwQ/8P3op5/Ym/FckAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since slope of the line for differences is not 1 there cannot be a constant d value but it will be a function of c. \n",
      "So c*=c-d(c) \n",
      "where d(c)=m*c+i where m= 0.8260433906216884 and i= -0.25426898002055953\n"
     ]
    }
   ],
   "source": [
    "# finding d for n- 2 to 9\n",
    "# removed 10 as outlier\n",
    "\n",
    "keys=list(adj_bigram_count.keys())\n",
    "keys=keys[8:0:-1]\n",
    "values=list(adj_bigram_count.values())\n",
    "values=values[8:0:-1]\n",
    "\n",
    "style.use('ggplot')\n",
    "xs = np.array(keys, dtype=np.float64)\n",
    "ys = np.array(values, dtype=np.float64)\n",
    "\n",
    "def best_fit_slope_and_intercept(xs,ys):\n",
    "    m = (((mean(xs)*mean(ys)) - mean(xs*ys)) /\n",
    "         ((mean(xs)*mean(xs)) - mean(xs*xs)))\n",
    "    \n",
    "    b = mean(ys) - m*mean(xs)\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "m, i = best_fit_slope_and_intercept(xs,ys)\n",
    "regression_line = [(m*x)+i for x in xs]\n",
    "\n",
    "plt.scatter(keys,values)\n",
    "plt.plot(xs, regression_line)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Adjusted Count\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Since slope of the line for differences is not 1 there cannot be a constant d value but\\\n",
    " it will be a function of c. \\nSo c*=c-d(c) \\nwhere d(c)=m*c+i where m=\",m,\"and i=\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(x):\n",
    "    y=m*x+b\n",
    "    return y\n",
    "\n",
    "for key, value in bigram_Nc.items():\n",
    "    if key not in [0,1]:\n",
    "        bigram_Nc[key]=m*value+i\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Good Turing Prob \n",
    "bigram_good_tur={}\n",
    "for key in rvrs_bigram_dict.keys():\n",
    "    for word in rvrs_bigram_dict[key]:\n",
    "        bigram_good_tur[word]=bigram_Nc[key]/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(data, model_name):\n",
    "    if type(data)!=type([]):\n",
    "        print(\"Data must be a list of sentences\")\n",
    "        return None\n",
    "    if type(data[0])!=type(\" \"):\n",
    "        print(\"Each item in list must be a sentence\")\n",
    "    \n",
    "    n_sent=len(data)\n",
    "    perplexity=0\n",
    "    if type(model_name)==dict:\n",
    "        \n",
    "        for sent in data:\n",
    "            word_list=sent.split(\" \")\n",
    "            N_grams=[]\n",
    "            \n",
    "            for i in range(len(word_list)-1):\n",
    "                n_gram=[]\n",
    "                for j in range(0,2):\n",
    "                    n_gram.append(words[i+j])\n",
    "                N_grams.append(\" \".join(n_gram))\n",
    "            N=0\n",
    "            sent_perplexity=1\n",
    "            \n",
    "            for bigram in N_grams:\n",
    "                N+=1\n",
    "                sent_perplexity=sent_perplexity*(1/model_name.get(bigram))\n",
    "            sent_perplexity=pow(sent_perplexity, 1/float(N))\n",
    "            \n",
    "            if float(\"Inf\")!=float(sent_perplexity):\n",
    "                perplexity=perplexity+sent_perplexity\n",
    "        \n",
    "        perplexity=perplexity/n_sent\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    else:\n",
    "        print(\"Model must be a dictonary of n-gram probabilities\")\n",
    "        return None\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity of Add_1 bigram model and good turing bigram trained on test_data\n",
    "add_1_perplexity=perplexity(test_data,bigram_add_1)\n",
    "gd_turing_perplexity=perplexity(test_data,bigram_good_tur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model              |   Perplexity |\n",
      "|--------------------+--------------|\n",
      "| Bigram Add 1       |   5517.86    |\n",
      "| Bigram Good Turing |      3.64232 |\n",
      "\n",
      "Models are trained on 21840 bigrams and perplexity is calculated on 5526. As it can be seen from the table the perplexity of the Good Turing somotthing is much lower than that of Add 1 . Hence Good Turing is a better model among the two.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (tabulate([[\"Bigram Add 1\",add_1_perplexity],[\"Bigram Good Turing\",gd_turing_perplexity]], headers=['Model','Perplexity'],tablefmt='orgtbl'))\n",
    "test_bigrams=n_gram_split(test_data,2)\n",
    "answer=\"Models are trained on \"+str(len(bigrams))+\" bigrams and perplexity is calculated on \"+str(len(test_bigrams))+\"\\\n",
    ". As it can be seen from the table the perplexity of the Good Turing somotthing is much lower than that of Add 1 \\\n",
    ". Hence Good Turing is a better model among the two.\"\n",
    "\n",
    "print(\"\\n\"+answer+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
